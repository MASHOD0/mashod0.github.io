<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Image Captioning Algorithm 2 | Mohammad Mashhood Alam</title> <meta name="author" content="Mohammad Mashhood Alam"> <meta name="description" content="Improving On image captioning algorithm to make the captions more social media friendly."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mashod0.github.io/projects/Image_caption_2/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mohammad¬†</span>Mashhood¬†Alam</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects &amp; Presentations</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Image Captioning Algorithm 2</h1> <p class="post-description">Improving On image captioning algorithm to make the captions more social media friendly.</p> </header> <article> <h1 id="image-caption-20">Image caption 2.0</h1> <h2 id="whats-new-">What‚Äôs New ?</h2> <ul> <li>Breakaway from the old simple descriptions to generating catchy captions for those instagram worthy posts <h3 id="how-did-i-do-it-">How did I do it ?</h3> <p><img src="https://user-images.githubusercontent.com/63853764/226140082-8a04cf67-c5d5-4a12-b058-bf8fff41c603.png" alt="image"></p> </li> <li>Used <code class="language-plaintext highlighter-rouge">Davinci-003</code> which is OpenAIs Large Language model based on the GPT-3.5 model architecture. <h3 id="how-is-this-better-">How is this better ?</h3> </li> <li>Organic looking captions for instagram or linkedin images <ul> <li>before : <code class="language-plaintext highlighter-rouge">this is a picture of a soccer player running with a soccer ball in his hand</code> </li> <li>after : <code class="language-plaintext highlighter-rouge">If you're not sweating, you're not working hard enough!</code> </li> </ul> </li> <li>Customized captions for each social media platform to get the right impact <ul> <li>Instagram: <code class="language-plaintext highlighter-rouge">If you're not sweating, you're not working hard enough!</code> </li> <li>LinkedIn: <code class="language-plaintext highlighter-rouge">There's nothing like a game of soccer to get the blood pumping!</code> </li> <li>Twitter: <code class="language-plaintext highlighter-rouge">He's running with a soccer ball‚Ä¶ and he's not looking back!</code> </li> </ul> </li> <li>Faster than than the previous version roughly <strong>7.2%</strong> faster than the previous version</li> <li>old outputs: <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>filename : Image1.png
this is a picture of a soccer player running with a soccer ball in his hand
there is a man that is running with a soccer ball in his hand
this is a picture of a soccer player running with the ball in his hand
this is a picture of a soccer player running with a soccer ball in front of him
there is a man that is running with a soccer ball in front of him
filename : Image2.png
there are two horses that are standing next to each other in a field
there are two horses that are standing next to each other in the middle of a field
there are two horses that are standing next to each other in the middle of the field
there are two horses that are standing next to each other in the field
there are two horses that are standing next to each other on a field
filename : Image3.png
this is an image of a group of people who are looking at each other
this is an image of a group of four people who are looking at each other
this is an image of a group of people who are looking at the camera
this is an image of a group of four people who are looking at the same time of the day
this is an image of a group of four people who are looking at the same time of day
execution time: 80.84076929092407
</code></pre></div> </div> </li> <li>new outputs ``` description: [‚Äòthere is a man that is running with a soccer ball in his hand‚Äô] platform: instagram captions: <ol> <li>Just another day on the pitch!</li> <li>Running with the ball ‚Äì gotta love soccer!</li> <li>Never give up on your dreams!</li> <li>If you‚Äôre not sweating, you‚Äôre not working hard enough!</li> <li>Soccer is my life!</li> </ol> </li> </ul> <p>description: [‚Äòthere are two horses that are standing next to each other in a field‚Äô] platform: instagram captions:</p> <ol> <li>‚ÄúBest friends forever!‚Äù</li> <li>‚ÄúThere‚Äôs no place like home.‚Äù</li> <li>‚ÄúA horse is a horse, of course, of course.‚Äù</li> <li>‚ÄúI‚Äôm a little horse of a different color.‚Äù</li> <li>‚ÄúWe‚Äôre two of a kind!‚Äù</li> </ol> <p>description: [‚Äòthis is an image of a group of people who are looking at each other‚Äô] platform: instagram captions:</p> <ol> <li>Connection is key.</li> <li>United we stand.</li> <li>strength in numbers</li> <li>A team that communicate well is a team that succeeds.</li> <li>Building relationships is the foundation of any successful venture. total tokens: 171 execution time: 75.04193019866943 ```</li> </ol> <h2 id="problem-statement">Problem Statement</h2> <ul> <li>Create an AI tool that creates captions based on the image provided by the user. Should also have the option to generate multiple captions based on the image.</li> <li>Provide an interface where the user can come and upload images and get AI generated captions.</li> </ul> <h2 id="solution">Solution</h2> <h3 id="pre-processing">Pre-Processing</h3> <ul> <li>Used the <a href="https://huggingface.co/docs/transformers/v4.26.1/en/model_doc/blip#transformers.BlipProcessor" rel="external nofollow noopener" target="_blank">Blip processor</a>, for processing the image. <h3 id="models">Models</h3> <h4 id="1-train-a--model-on-the-fliker8k-dataset">1. Train a model on the Fliker8k dataset</h4> </li> <li>Train a image captioning model using CNN and Transformer architecture on the Fliker8k dataset.</li> <li> <a href="https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip" rel="external nofollow noopener" target="_blank">Fliker8k</a> dataset contains 8092 images and 5 captions for each image.</li> <li>Disadvantages: <ul> <li>The dataset is small and the model will not be able to generalize well.</li> <li>The model will not be able to generate captions for images that are not present in the dataset.</li> </ul> </li> </ul> <h4 id="2-the-nlpconnectvit-gpt2-image-captioning-model">2. The <code class="language-plaintext highlighter-rouge">nlpconnect/vit-gpt2-image-captioning</code> model</h4> <ul> <li>Use the <code class="language-plaintext highlighter-rouge">nlpconnect/vit-gpt2-image-captioning</code> model to generate captions for the images.</li> <li> <a href="https://huggingface.co/nlpconnect/vit-gpt2-image-captioning" rel="external nofollow noopener" target="_blank">nlpconnect/vit-gpt2-image-captioning</a> model is one of the most downloaded model on the huggingface model hub. It has over 1.1 million downloads.</li> <li><a href="nlpconnect-vit-gpt-2-captioning.py">the script used in generating the results</a></li> </ul> <h5 id="results-of-the-nlpconnectvit-gpt2-image-captioning-model">Results of the <code class="language-plaintext highlighter-rouge">nlpconnect/vit-gpt2-image-captioning</code> model</h5> <ul> <li> <code class="language-plaintext highlighter-rouge">temprature</code> adjustments : (https://docs.google.com/spreadsheets/d/1yz25PL-s2VbGVhij0wr9SMotuXexYrgGU-svi4aFIic/edit?usp=sharing)</li> <li>experimentation with <code class="language-plaintext highlighter-rouge">top_k</code> parameter: (https://docs.google.com/spreadsheets/d/1Wfhxj-4AX5WQpGO2v_0B9e0NspoHOsERJw229llngbY/edit?usp=sharing)</li> <li>results of the full test of selected <code class="language-plaintext highlighter-rouge">temprature</code> and <code class="language-plaintext highlighter-rouge">top_k</code> values:(https://docs.google.com/spreadsheets/d/1Gb1XxMa3S2hSjamPjVTGfNRNz83Ff67HWuOXzQstEIU/edit?usp=sharing) <img src="https://user-images.githubusercontent.com/63853764/221746440-deb4823f-bdb0-4275-8938-343d1da2b53c.png" alt="image"> </li> <li>The generated sentences were not good enough.</li> </ul> <h4 id="3-the--salesforce-blip-image-captioning-large-model">3. The ` Salesforce Blip image captioning large` model</h4> <ul> <li> <strong>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</strong> is a pre-trained model for image captioning.</li> <li>The model is trained on the <a href="https://cocodataset.org/#home" rel="external nofollow noopener" target="_blank">MS COCO</a> dataset.</li> <li>The model is trained on the <code class="language-plaintext highlighter-rouge">large</code> version of the <a href="https://arxiv.org/abs/2010.11929" rel="external nofollow noopener" target="_blank">ViT</a> architecture.</li> <li>This model is selected for the Implementation as the captions generated from this are good</li> <li><a href="salesforce_blip_image_captioning_large.py">Script used for experimentation </a></li> </ul> <h3 id="model-evaluation">Model evaluation</h3> <ul> <li>Simple intuition was used here to evaluate the outputs of various models as the difference between the accuracy of the models was clearly visible. <img src="https://user-images.githubusercontent.com/63853764/221748485-4c1a2481-679a-48e2-b69a-8c4ca3454864.png" alt="image"> </li> <li>For example, for the above image the following are sentences generated by <code class="language-plaintext highlighter-rouge">nlpconnect/vit-gpt2-image-captioning</code> and ` Salesforce Blip image captioning large` models <ul> <li> <code class="language-plaintext highlighter-rouge">nlpconnect/vit-gpt2-image-captioning</code>: a woman in brown outfit holding a white horse behind it in dark cloudy sky</li> <li>` Salesforce Blip image captioning large` : there are two horses that are standing next to each other in a field</li> </ul> </li> </ul> <h3 id="interface">Interface</h3> <h4 id="1cli">1.CLI</h4> <ul> <li>A Simple command line application that can be run by running <a href="/runner.py"><code class="language-plaintext highlighter-rouge">runner.py</code></a> </li> <li>Prompts the user for location of the folder and number of captions to generate.</li> <li>Generates the captions and calculates the time taken to generate the results.</li> <li>Suitable for batch generation of captions as it can take multiple images at once</li> <li>Selected for implementation <h4 id="2gui">2.GUI</h4> </li> <li>A web application implemented using frameworks like flask.</li> <li>Prompts the user to upload the files, generates and displays the captions on the web interface.</li> <li>Requires UI Design and file handling.</li> </ul> <h3 id="implementation">Implementation</h3> <h4 id="data-files">Data Files</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>üì¶listed_image_caption
 ‚îÉ 
 ‚îó üìÇinputs
   ‚î£ üìúImage1.png
   ‚î£ üìúImage2.png
   ‚îó üìúImage3.png
 
</code></pre></div></div> <p>#### Code files</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>üì¶listed_image_caption
 ‚îÉ
 ‚î£ üìúModel.py
 ‚î£ üìúnlpconnect-vit-gpt-2-captioning.py
 ‚î£ üìúrunner.py
 ‚îó üìúsalesforce_blip_image_captioning_large.py
</code></pre></div></div> <p>##### <code class="language-plaintext highlighter-rouge">Model.py</code> Implements the <code class="language-plaintext highlighter-rouge">predict()</code> which takes the image and number of sequences as input and generates the captions. ##### <code class="language-plaintext highlighter-rouge">runner.py</code> Implements the interface of the application. ##### <code class="language-plaintext highlighter-rouge">nlpconnect-vit-gpt-2-captioning.py</code> Experiments with the nlpconnect-vit-gpt-2-captioning model. ##### <code class="language-plaintext highlighter-rouge">salesforce_blip_image_captioning_large.py</code> Experiments with the salesforce_blip_image_captioning_large model</p> <h2 id="improvements-and-conclusion">Improvements and Conclusion</h2> <ul> <li>Create a GUI user interface using flask or tkinter, this will make the application more user friendly.</li> <li>The model can be improved by using a larger dataset and training it for a longer period of time.</li> <li>The runtimes can be improved by using a GPU.</li> <li>Using metrics like BLEU score for model evaluation.</li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 Mohammad Mashhood Alam. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>